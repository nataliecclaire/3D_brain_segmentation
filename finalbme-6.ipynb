{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2643566,"sourceType":"datasetVersion","datasetId":1607477}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Final BME Project: 3D Brain Segmentation","metadata":{}},{"cell_type":"markdown","source":"Natalie McKenzie - ncm2165\nEva Melendrez - emm2355\n\nWelcome to our Final Biomedical Engineering Project! In this Kaggle notebook, we will walk you through the process of building a model that segments different parts of the brain in a MRI T1 (3D) scan of a brain. Firstly, let's import our libraries and our raw data!","metadata":{}},{"cell_type":"code","source":"#Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport warnings\n\nimport nibabel as nib\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.callbacks import LambdaCallback\nfrom tensorflow.keras import layers, models\nimport os\nimport tensorflow as tf\nfrom scipy.ndimage import zoom\nimport skimage.transform as skTrans\n#1: import packages\nimport sys, os\nimport glob\nimport math\nimport cv2\nimport scipy.interpolate as scInterp\nfrom torch.optim import Adam, SGD\n\ncuda = torch.cuda.is_available()\nprint (\"GPU available:\", cuda)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Masks we want from our data - background, cerebrospinal fluid, grey matter, and white matter\nmasks = [\"background\", \"csf\", \"gm\", \"wm\"]\n\ndef plot_image(mask_array, image_array, idx, axis, save_root_folder=None):\n\n    plt.figure(figsize=(22, 22))\n    plt.subplot(1, 5, 1)\n    plt.title(f\"{idx} image on Axis {axis}\")\n    plt.imshow(image_array[0], cmap=\"gray\")\n    \n    for i in range(len(masks)):\n        show_image = mask_array[i]\n        plt.subplot(1, 5, i + 2)\n        plt.title(f\"{idx} {masks[i]} on Axis {axis}\")\n        plt.imshow(show_image, cmap=\"gray\")\n        plt.axis(\"off\")\n    \n    plt.tight_layout()\n    \n    if save_root_folder:\n        plt.savefig(os.path.join(save_root_folder, f\"Axis_{axis}_{idx}_Images_And_Masks.png\"))\n    \n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef niftyToNumpy(root_folder, folder, output_folder):\n    folder_path = os.path.join(root_folder, folder)\n    img_folder_path = os.path.join(folder_path, 'image')\n    mask_folder_path = os.path.join(folder_path, 'mask')\n    output_folder_path = os.path.join(output_folder, folder)\n    \n    os.makedirs(output_folder_path, exist_ok=True)\n    \n    sbj_list = [i.split(\"_img.nii\")[0] for i in os.listdir(img_folder_path) if i.endswith(\".nii\")]\n    \n    total_images = 0\n    \n    for sbj in sbj_list:\n        print(f\"Currently processing {sbj} in {folder}, {sbj_list.index(sbj) + 1} of {len(sbj_list)}\", end=\"\\r\")\n        \n        img = nib.load(os.path.join(img_folder_path, sbj + \"_img.nii\")).get_fdata()\n        # This line will normalize the pixel intensity to a [0, 1] range\n        img = (img - img.min()) / (img.max() - img.min())\n        mask_csf = nib.load(os.path.join(mask_folder_path, sbj + \"_probmask_csf.nii\")).get_fdata()\n        mask_gm = nib.load(os.path.join(mask_folder_path, sbj + \"_probmask_graymatter.nii\")).get_fdata()\n        mask_wm = nib.load(os.path.join(mask_folder_path, sbj + \"_probmask_whitematter.nii\")).get_fdata()\n        \n        # Create combined mask array\n        mask = np.zeros((4,) + img.shape)\n        mask[1] = mask_csf\n        mask[2] = mask_gm\n        mask[3] = mask_wm\n        mask[0] = np.logical_and(mask[1] == 0, np.logical_and(mask[2] == 0, mask[3] == 0)).astype(np.float32)\n        \n        # Resize image and mask\n        img = skTrans.resize(np.expand_dims(img, axis=0), (1, 112, 112, 112), order=1, preserve_range=True)\n        mask = skTrans.resize(mask, (4, 112, 112, 112), order=1, preserve_range=True)\n        \n        # Iterate over slices in the middle of the image (to avoid empty slices)\n        for idx in range(1, img.shape[1], 7):  # Adjust as necessary\n            img_slc = img[:, idx, :, :]\n            mask_slc = mask[:, idx, :, :]\n            \n            total_images += 1\n            \n            # If the current subject is \"sald_031318\" in the \"test\" folder, visualize the image and masks\n            if sbj == \"sald_031318\" and folder == \"test\":\n                plot_image(mask_slc, img_slc, idx, axis=\"Y\", save_root_folder=None)\n            \n            # Save the image and mask slices\n            np.save(os.path.join(output_folder_path, f\"{sbj}_slc{idx:03}_img.npy\"), img_slc)\n            np.save(os.path.join(output_folder_path, f\"{sbj}_slc{idx:03}_mask.npy\"), mask_slc)\n    \n    print(f\"\\nTotal images processed: {total_images}\")\n\n# Define paths\nog_folder = '/kaggle/input/3dbraintissuesegmentation'  # Path to the original dataset\nnp_folder = './processed_data'  # Path where you want to save the processed data (can be any directory)\n\n# Create processed data folder if it doesn't exist\nos.makedirs(np_folder, exist_ok=True)\n\n\nfolders = ['test', 'train', 'valid']\nfor curr_folder in folders:\n    niftyToNumpy(og_folder, curr_folder, np_folder)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BasicDataset(torch.utils.data.Dataset):\n    def __init__(self, folder, split, percent_samples=100):\n        self.folder = folder\n        self.split = split\n        self.ids = self.load_ids(folder, split)\n        # Optionally limit the number of samples if needed\n        self.num_samples = int(len(self.ids) * percent_samples / 100)\n    \n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        # Ensure you are not slicing along axes, just load the whole image data\n        image_file = os.path.join(self.folder, self.split, \"image\", self.ids[idx] + \"_img.npy\")\n        mask_file = os.path.join(self.folder, self.split, \"mask\", self.ids[idx] + \"_mask.npy\")\n        \n        # Load the image and mask data\n        image = np.load(image_file)\n        mask = np.load(mask_file)\n        \n        # Return the whole image and mask\n        return {'image': image, 'mask': mask}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Above, you can see what the MRI scans look like after we conduct the 2D array transformation of the 3D images. We utilized images from the test set so you can see clearly what we expect from our model's performance. It should be able to determine wher ethe background, cerebrospinal fluid, grey matter, and white matter are in one of the MRI scans, as shown divided up above.\n\nWe included a counting method so we can ensure that the number of images we enter with in the transformation is the same number we get coming out. Next, we're going to implement the U-Net structure with transposed convolution to upsample our data and to segment it.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n#Conv 3x3, ReLU\nclass ReLUConv(nn.Module):\n    \"\"\"(2d convolution, batch norm, relu) x 2\"\"\"\n    def __init__(self, ch_in, ch_out):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            #Shrink by 2 pixels a second time\n            nn.Conv2d(ch_in, ch_out, kernel_size = 3, padding = 0),\n            nn.BatchNorm2d(ch_out), \n            nn.ReLU(inplace = True),\n            #Shrink by 2 pixels second time\n            nn.Conv2d(ch_out, ch_out, kernel_size = 3, padding = 0),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace = True)\n        )\n    def forward(self, x):\n        return self.double_conv(x)\n\n#Copy and Crop: TO STUDY LATER\n\n#Max pooling/Down-Conv 2x2\nclass DownConv(nn.Module):\n    \"\"\"maxpool, DoubleConv\"\"\"\n    def __init__(self, ch_in, ch_out):\n        super().__init__()\n        self.down_conv = nn.Sequential(\n            #Reduce dimensions by half of what they were before\n            nn.MaxPool2d(2),\n            ReLUConv(ch_in, ch_out)\n        )\n    def forward(self, x):\n        return self.down_conv(x)\n\n#Up-Conv 2x2\nclass UpConv(nn.Module):\n    \"\"\"upsample, 1x1 conv, concatenate with earlier layer, DoubleConv\"\"\"\n    def __init__(self, ch_in, ch_out):\n        super().__init__()\n        self.up_conv = nn.Sequential(\n            nn.Upsample(scale_factor = 2, mode = \"bilinear\", align_corners = True),\n            nn.Conv2d(ch_in, ch_out, kernel_size = 1)\n        )\n        self.conv = DoubleConv(ch_out * 2, ch_out)\n    def forward(self, x1, x2):\n        x1 = self.up_conv(x1)\n        x = torch.cat([x1, x2], dim = 1)\n        x = self.conv(x)\n        return x\n\n#Conv 1x1\nclass OutConv(nn.Module):\n    \"\"\"1x1 conv, softmax (last operation)\"\"\"\n    def __init__(self, ch_in, ch_out):\n        super().__init__()\n        self.conv_final = nn.Sequential(\n            nn.Conv2d(ch_in, ch_out, kernel_size = 1),\n            nn.Softmax(dim = 1)\n        )\n    def forward(self, x):\n        return self.conv_final(x)\n   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ThisCNN(nn.Module):\n    def __init__(self, name, n_channels, n_classes):\n        super().__init__()\n        self.name = name\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n\n        #Structure copied from class slides\n        self.inputL = ReLUConv(n_channels, 64)\n        self.down1 = DownConv(64, 128)\n        self.down2 = DownConv(128, 256)\n        self.down3 = DownConv(256, 512)\n        self.down4 = DownConv(512, 1024)\n        self.up1 = UpConv(1024, 512)\n        self.up2 = UpConv(512, 256)\n        self.up3 = UpConv(256, 128)\n        self.up4 = UpConv(128, 64)\n        self.outputL = OutConv(64, n_classes)\n        \n    def forward(self, x):\n        x = self.inputL(x)\n        \n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        \n        #wow look at that u-net concatenating\n        u1 = self.up1(d4, d3)\n        u2 = self.up2(u1, d2)\n        u3 = self.up3(u2, d1)\n        u4 = self.up4(u3, x)\n        \n        x = self.outputL(u4)\n        \n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_net(net, epochs, train_dataloader, valid_dataloader, optimizer, loss_function):\n    \n    #create folder to model in (if not already existing)\n    if not os.path.isdir(f'{net.name}'):\n        os.mkdir(f'{net.name}')\n    \n    n_train = len(train_dataloader)\n    n_valid = len(valid_dataloader)    \n    \n    train_loss = []\n    train_pearson = []\n    \n    valid_loss = []\n    valid_pearson = []\n    \n    #TRAINING\n    for epoch in range(epochs):\n        net.train()\n        \n        train_batch_loss = []\n        train_batch_pearson = []\n        \n        for i, batch in enumerate(train_dataloader):\n            \n            #load image and gt mask for the batch and mount to cupa \n            imgs = batch['image'].cuda()\n            masks = batch['mask'].cuda()\n\n            #load model prediction w/ current weights\n            pred = net(imgs)\n\n            #compute batch loss and append to overall training loss\n            loss, isolated_images, stacked_brain_map = loss_function(criterion, imgs, pred, masks, 'train')\n            batch_loss = loss.item() #this just turns it into a number instead (i think it was a tensor)\n            train_batch_loss.append(batch_loss)\n\n            #compute batch's pearson coeff and append to overall\n            batch_pearson = pearson_coeff(isolated_images, masks, stacked_brain_map)\n            train_batch_pearson.append(batch_pearson)\n\n            optimizer.zero_grad() #reset gradient values\n            loss.backward() #compute backwards loss\n            optimizer.step() #update weights\n            \n            #update progress (bc the end = '\\r' rewrites over the last!)\n            print(f'EPOCH {epoch + 1}/{epochs} - Training Batch {i+1}/{n_train} - Loss: {batch_loss}, Pearson Coefficient: {batch_pearson}', end='\\r')\n        \n        #calculate averages and append to overall\n        average_training_loss = np.array(train_batch_loss).mean()\n        average_training_pearson = np.array(train_batch_pearson).mean()\n\n        train_loss.append(average_training_loss)\n        train_pearson.append(average_training_pearson)\n        \n        #VALIDATION\n        net.eval()\n        valid_batch_loss = list()\n        valid_batch_pearson = list()\n        \n        #don't calculate gradient since you're not trying to train here, just evaluate\n        with torch.no_grad():\n            for i, batch in enumerate(valid_dataloader):\n\n                #load image and mask for batch, mount to cupa\n                imgs = batch['image'].cuda()\n                masks = batch['mask'].cuda()\n\n\n                #load model prediction w/ current weights\n                pred = net(imgs)\n\n                # compute batch loss and append to overall validation loss\n                loss, isolated_images, stacked_brain_map = loss_function(criterion, imgs, pred, masks, 'val')\n                batch_loss = loss.item()\n                valid_batch_loss.append(batch_loss)\n\n                #compute batch's pearson coeff and append to overall validation loss\n                batch_pearson = pearson_coeff(isolated_images, masks, stacked_brain_map)\n                valid_batch_pearson.append(batch_pearson)\n\n                #update progress (bc the end = '\\r' rewrites over the last!)\n                print(f'EPOCH {epoch + 1}/{epochs} - Validation Batch {i+1}/{n_valid} - Loss: {batch_loss}, Pearson Coefficient: {batch_pearson}', end='\\r')\n                \n        average_validation_loss = np.array(valid_batch_loss).mean()\n        average_validation_pearson = np.array(valid_batch_pearson).mean()\n        valid_loss.append(average_validation_loss)\n        valid_pearson.append(average_validation_pearson)\n        \n        #print final evaluation of epoch \n        print(f'EPOCH {epoch + 1}/{epochs} - Training Loss: {average_training_loss}, Training Pearson score: {average_training_pearson}, Validation Loss: {average_validation_loss}, Validation Pearson Coefficient: {average_validation_pearson}')\n\n        #save model \n        #note: since there's only 50 epochs you can change to {:02}\n        torch.save(net.state_dict(), f'{net.name}/epoch_{epoch+1:03}.pth')\n    \n    return train_loss, train_pearson, valid_loss, valid_pearson","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def loss_function(criterion, images, model_output, gt_masks, mode):\n    \"\"\"\n    calculate loss for the three tissue type channels only over the brain area\n    compute loss for background channel over the whole area\n    \"\"\"\n    \n    if mode == \"val\":\n        torch.manual_seed(1102)\n        np.random.seed(1102)\n        \n    #remove extra dimension\n    images_squeezed = torch.squeeze(images, dim = 1)\n    \n    #turn this into binary brain map (values are 1 where brain, 0 where background)\n    brain_map = (images_squeezed > 0).float()\n    \n    #stack the brain maps so their shape is batch_size x 4 x 112 x 112 (like model's output)\n    #computing loss for background channel over the whole area, so that will just be filled with ones \n    ones = torch.ones((brain_map.shape)).cuda()\n    stacked_brain_map = torch.stack(([ones, brain_map, brain_map, brain_map]), dim = 1)\n    \n    #only consider the values inside the brain (zero out others using 0s in bg of stacked brain map)\n    isolated_images = torch.mul(stacked_brain_map, model_output)\n\n    #see how the probabilities at each (prediction vs ground truth) compare using whatever criterion you prefer\n    #computes avg loss over all the brain voxels\n    loss = criterion(isolated_images, gt_masks) #make sure reduction = \"sum\" so you can do average over entire brain area\n    num_brain_voxels = stacked_brain_map.sum()\n    loss = loss / num_brain_voxels\n    \n    #return the isolated_images and the stacked_brain_map too because they're helpful for other functions\n    return loss, isolated_images, stacked_brain_map\n    \ndef pearson_coeff(isolated_image, target, stacked_brain_map):\n    \"\"\"\n    calculate pearson correlation coefficient over the brain area\n    \"\"\"\n    #flatten so you can do pearson correlation calculation\n    gt_flattened = torch.flatten(target)\n    iso_flattened = torch.flatten(isolated_image)\n    mask_flattened = torch.flatten(stacked_brain_map)\n    \n    #only keep the values where there is brain (like not just zero them out, but discard them)\n    gt_flattened = gt_flattened[mask_flattened.nonzero(as_tuple = True)]\n    iso_flattened = iso_flattened[mask_flattened.nonzero(as_tuple = True)]\n    \n    #remove from cupa and turn into numpy array\n    iso_flattened = iso_flattened.cpu().detach().numpy()\n    gt_flattened = gt_flattened.cpu().detach().numpy()\n\n    \n    #calculate and return pearson correlation coefficient\n    pearson = np.corrcoef(iso_flattened, gt_flattened)[0][1]\n    return pearson","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_coeff_CM(isolated_image, target, stacked_brain_map):\n    \"\"\"\n    calculate dice coefficient to evaluate model in testing stage and also return confusion matrix\n    literally just convenient to do them together since they need the same inputs and you only need either for the testing stage\n    not going to include bacground in calcations because there is so much background, it'll make the dice score higher than it should be\n    \"\"\"\n    \n    #create dictionary for each channel\n    #ie. gt1 would csf, 2 gm, 3 wm\n    masks_list = [\"gt1\", \"gt2\", \"gt3\", \"iso1\", \"iso2\", \"iso3\"]\n    masks_dict = {i :{} for i in masks_list}\n    \n    #create maps with values of 0, 1, 2, 3, depending on which tissue type is the most likely to exist there (max value)\n    #reminder: 0 is background here\n\n    full_map_model = torch.argmax(isolated_image, 1) \n    full_map_gt = torch.argmax(target, 1)\n    #shape of these is batch_size x 122 x 122\n\n    binary_brain = stacked_brain_map[:,1,:,:]\n    brain_flattened = torch.flatten(binary_brain)\n     \n    #okay quick pause for confusion matrix calculations\n    \n    #flatten because that's needed for CM calculations\n    CM_full_map_model = torch.flatten(full_map_model)\n    CM_full_map_gt = torch.flatten(full_map_gt)\n    \n    #detach from cupa and turn into numpy arrays for CM calculation\n    CM_full_map_model = CM_full_map_model.cpu().detach().numpy()\n    CM_full_map_gt = CM_full_map_gt.cpu().detach().numpy()\n    CM = confusion_matrix(CM_full_map_gt, CM_full_map_model)\n    \n    #okay back to dice_scores\n    dice_scores = []\n    \n    #calculate one dice score for each tissue type\n    for i in range (1, 4):\n        #key is a tensor where 0 if not correct tissue type (background or other tissue), 1 if correct tissue type\n        masks_dict[\"gt\" + str(i)] = (full_map_gt == i).float()\n        masks_dict[\"iso\" + str(i)] = (full_map_model == i).float()\n        masks_dict[\"gt\" + str(i)] = torch.flatten(masks_dict[\"gt\" + str(i)])\n        masks_dict[\"gt\" + str(i)] = masks_dict[\"gt\" + str(i)][brain_flattened.nonzero(as_tuple = True)]\n        masks_dict[\"gt\" + str(i)] = masks_dict[\"gt\" + str(i)].cpu().detach().numpy()\n        \n        masks_dict[\"iso\" + str(i)] = torch.flatten(masks_dict[\"iso\" + str(i)])\n        masks_dict[\"iso\" + str(i)] = masks_dict[\"iso\" + str(i)][brain_flattened.nonzero(as_tuple = True)]\n        masks_dict[\"iso\" + str(i)] = masks_dict[\"iso\" + str(i)].cpu().detach().numpy()\n    \n        #okay wait actually none of the mask_dict stuff is really necessary since you're not accessing the dictionaries later...\n        #like you're literally taking these values and assigning them new names...\n        #note: come back and delete all the dictionary stuff... literally just give them the below names from the beginning\n        model_output = masks_dict[\"iso\" + str(i)]\n        gt = masks_dict[\"gt\" + str(i)]\n        eps = 0.00001 #so there's no dividing by zero\n        \n        #calculate dice scores and append to overall dice scores list\n        dice = (np.sum(model_output[gt == 1]) * 2.0 + eps) / (np.sum(model_output) + np.sum(gt) + eps)\n        dice_scores.append(dice)\n    \n    #calculate and return avg dice scores and confusion matrix info\n    avg_dice = sum(dice_scores)/len(dice_scores)\n    return avg_dice, CM\n\n#TESTING\ndef test_net(net, test_dataloader, loss_function):\n    if not os.path.isdir('/kaggle/working/pred_mask'):\n        os.mkdir('/kaggle/working/pred_mask')\n    net.eval()\n    \n    n_test = len(test_dataloader)\n    test_batch_loss = []\n    test_batch_pearson = []\n    test_batch_dice = []\n    test_batch_CM = []\n    all_imgs = []\n    all_labels = list()\n    all_preds = list()\n    \n    #no need to update gradient\n    with torch.no_grad():\n        for i, batch in enumerate(test_dataloader):\n\n            #load image and mask for batch, mount to cupa\n            imgs = batch['image'].cuda()\n            masks = batch['mask'].cuda()\n            \n            #load model prediction w/ \"best\" weights\n            pred = net(imgs)\n            \n            #compute batch loss and append to overall validation loss\n            loss, isolated_images, stacked_brain_map = loss_function(criterion, imgs, pred, masks, 'test')\n            batch_loss = loss.item()\n            test_batch_loss.append(batch_loss)\n            \n            #compute batch's pearson coeff and append to overall validation loss            \n            batch_pearson = pearson_coeff(isolated_images, masks, stacked_brain_map)\n            test_batch_pearson.append(batch_pearson)\n            \n            #compute batch's dice score and confusion matrix and append to overall lists\n            batch_dice, batch_CM = dice_coeff_CM(isolated_images, masks, stacked_brain_map)\n            test_batch_dice.append(batch_dice)\n            test_batch_CM.append(batch_CM)\n            \n            pred = pred.cpu().detach().numpy()\n            imgs = imgs.cpu().detach().numpy()\n            labels = batch['sbj_id']\n            all_labels+=labels\n            all_preds.append(pred)\n            all_imgs.append(imgs)\n            \n            #update progress\n            print(f'Test Batch {i+1}/{n_test} - Loss: {batch_loss}, Pearson Corr: {batch_pearson}, DICE score: {batch_dice}', end='\\r')\n        \n        #find averages of all metrics and return them\n        test_loss = np.array(test_batch_loss).mean()\n        test_dice = np.array(test_batch_dice).mean()\n        test_pearson = np.array(test_batch_pearson).mean()\n        test_CM = np.array(test_batch_CM).mean(axis = 0)\n        \n    return all_imgs, all_labels, all_preds, test_loss, test_dice, test_pearson, test_CM","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def CM(test_CM, save_root_folder, label):\n    \"\"\"\n    given CM values, return a nice little visualization of them :), but with the background values removed because there's too many \n    \"\"\"\n    \n    #turn into panda dataframe so you can add labels!\n    df_cm = pd.DataFrame(test_CM, index = [\"Background\", \"CSF\", \"GM\", \"WM\"],\n                     columns = [\"Background\", \"CSF\", \"GM\", \"WM\"])\n    \n    #remove the background >:(\n    df_cm.pop(\"Background\")\n    df_cm = df_cm.drop(\"Background\", axis = \"index\")\n    \n    #plot \n    plt.figure(figsize = (12,10))\n    plt.title('Confusion Matrix')\n    sns.heatmap(df_cm, annot = True, annot_kws = {\"size\": 15})\n    plt.ylabel('True labels')\n    plt.xlabel('predicted labels')\n    \n    #save plot\n    plt.savefig(os.path.join(save_root_folder, f\"{label}_Confusion_Matrix.png\"))\n    plt.show()\n\ndef learning_curve(best_epoch, train_loss, valid_loss, train_pearson, valid_pearson, save_root_folder):\n    \"\"\"\n    return visualization of how the metrics improved during each epoch (learning curve)\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(figsize = (15, 8), ncols = 2)\n    fig.suptitle(\"Learning Curve\", fontsize = 18)\n        \n    #plot losses on first graph\n    ax1.set_ylabel(\"Loss\", fontsize = 15)\n    ax1.set_xlabel(\"Epoch\", fontsize = 15)\n    ax1.set_xticks(np.arange(EPOCHS, step = 10) + 1)\n    ax1.plot(np.arange(EPOCHS) + 1, train_loss, '-o', label = \"Training\")\n    ax1.plot(np.arange(EPOCHS) + 1, valid_loss, '-o', label = \"Validation\")\n    ax1.axvline(best_epoch, color = 'm', lw = 4, alpha = 0.5, label = \"Best Epoch\") #highlight the best epoch\n    \n    #plot pearson coefficients on second graph    \n    ax2.set_ylabel(\"Pearson Coeff\", fontsize = 15)\n    ax2.set_xlabel(\"Epoch\", fontsize = 15)\n    ax2.set_xticks(np.arange(EPOCHS, step = 10) + 1)\n    ax2.plot(np.arange(EPOCHS) + 1, train_pearson, '-o', label = \"Training\")\n    ax2.plot(np.arange(EPOCHS) + 1, valid_pearson, '-o', label = \"Validation \")\n    ax2.axvline(best_epoch, color = 'm', lw = 4, alpha = 0.5, label = \"Best Epoch\") #highlight the best epoch\n    \n    plt.legend()\n    plt.tight_layout()\n    \n    #save plot\n    plt.savefig(os.path.join(save_root_folder, \"Learning_Curve.png\"))\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not os.path.isdir(\"figures\"):\n    os.mkdir(\"figures\")\nsave_root_folder = \"/kaggle/working/figures\"\nfinal_predictions = list()\nfinal_labels = list()\nfinal_images = list()\n\n\ntorch.manual_seed(1102)\nnp.random.seed(1102) #Q: should i reset the seeds each time?\nroot_folder = f\"/kaggle/working/figures/train/\"\n\n#2: datasets and dataloaders\npercent = 1\ntrain_dataset = BasicDataset(root_folder, 'train', percent)\nvalid_dataset = BasicDataset(root_folder, 'valid', percent)\ntest_dataset = BasicDataset(root_folder, 'test', percent)\n\nbatch_size = 85\ntrain_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\nvalid_loader = DataLoader(valid_dataset, batch_size = batch_size)\ntest_loader = DataLoader(test_dataset, batch_size = batch_size)\n    \nprint (f\"Batches in Train DataLoader: {len(train_loader)}\")\nprint (f\"Batches in Validation DataLoader: {len(valid_loader)}\")\nprint (f\"Batches in Test DataLoader: {len(test_loader)}\")\n\n    #display the first five image slices\n#     for slc in range (5):\n#         plot_image(train_dataset[slc]['mask'], train_dataset[slc]['image'], train_dataset[slc]['sbj_id'], axis, save_root_folder)\n    \n    #create instance of u-net architecture\nmodel = ThisCNN(f\"Model_Axis_Y\", 1, 4)\nmodel = model.cuda()\n    \n        \n    #4: define loss function and optimization method\n    #define optimizer \noptimizer = torch.optim.SGD(model.parameters(), lr=0.005)\n    \n\n    #define loss function\ncriterion = nn.MSELoss(reduction = \"sum\")\n    \n    \n    #5: train the model for at least 50 epochs \n    #6: validate and save the model at the end of each epoch\nEPOCHS = 50\ntrain_loss, train_pearson, valid_loss, valid_pearson = train_net(model, EPOCHS, train_loader, valid_loader, optimizer, loss_function)\n\n    \n    #7: choose the best epoch and load the weights at that specific epoch\nbest_epoch = np.argmax(valid_pearson) + 1 #just add one bc epochs start at 1\nprint (f\"Best Epoch: {best_epoch}\")\n\n    #load weights to model\nstate_dict = torch.load(f'./Model_Axis_Y/epoch_{best_epoch:03}.pth')\n    torch.save(state_dict, f\"/kaggle/working/Axis_Y_model_best_epoch.pth\")\n    !rm -r './Model_Axis_{axis}'\n    model.load_state_dict(state_dict)\n    model.cuda()\n    \n\n    #8: test the model on the test data\n    test_imgs, test_labels, test_predictions, test_loss, test_dice, test_pearson, test_CM = test_net(model, test_loader, loss_function)\n    final_predictions.append(test_predictions)\n    final_labels.append(test_labels)\n    final_images.append(test_imgs)\n    print(f'Test Loss: {test_loss}, Test DICE score: {test_dice}, Test Pearson Correlation: {test_pearson}')\n    \n    for slc in range (3, 5):\n        plot_image(test_predictions[0][slc], test_imgs[0][slc], test_labels[0][slc], axis)\n\n    \n    #9: generate and save all the figures you might need for the presentation (dataset distribution, learning curve, confusion matrix, etc.)\n    learning_curve(best_epoch, train_loss, valid_loss, train_pearson, valid_pearson, save_root_folder)\n    CM(test_CM, save_root_folder, f\"Axis {axis}\")\n                    \n    model = model.cpu() #remove old model from gpu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we're going to normalize the MRI scans so that the voxels values are scaled from 0 to 1. This way, we can standardize every image's intensity range similarly, so that the model can learn more easily.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}